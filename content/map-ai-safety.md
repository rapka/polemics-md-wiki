# Rationality Cartography (AI Safety Edition)

![Map of AI Existential Safety](../images/maps/2023/map_2023_full.png)
Image from https://aisafety.info. This map is also catalogued on [AI Panic](https://www.aipanic.news/p/ultimate-guide-to-ai-existential). Descriptions of each entity are taken from the original website created by Dr. Niri Weiss-Blatt.

### Contents

#### **Funding**

- [Open Philanthropy](pages/Open%20Philanthropy.md) - The largest funder in the existential safety space. See the undergraduate scholarship and early career funding.


- [LTFF – Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) - The main source of grants for individuals working on AI safety.
- [SFF – Survival and Flourishing Fund](http://survivalandflourishing.fund/) - The second largest funder in AI safety, using an algorithm and meeting structure called “The S-process” to allocate grants.

[Lightspeed Grants](https://lightspeedgrants.org/) - Fast funding for projects that help humanity flourish among the stars.
- [GWWC – Giving What We Can](https://www.givingwhatwecan.org/) - A community of donors who have pledged to donate a significant portion of their income. Hosts annual donor lotteries.
- [FLI – Future of Life Institute – Fellowships](https://futureoflife.org/our-work/grantmaking-work/) - PhD and postdoc funding for work improving the future.
- [Longview Philanthropy](https://www.longview.org/) - Designs and executes bespoke giving strategies for major donors.
- [Manifund](https://manifund.org/) - A regranting and impact certificates platform for AI safety and other EA activities.
- [CLR – Center on Long-Term Risk – Fund](https://longtermrisk.org/grantmaking/) - Financial support for projects focused on reducing (current and future) s-risks.
- [AI Safety GiveWiki](https://ai.givewiki.org/) (Formerly Impact Markets) - Crowdsourced charity evaluator focused on early-stage projects.
- [EAIF – Effective Altruism Infrastructure Fund](https://funds.effectivealtruism.org/funds/ea-community) - Support for EA infrastructure, including AI safety infrastructure.
- [AI2050](https://www.schmidtfutures.com/our-work/ai2050/) - Philanthropic initiative supporting researchers working on key opportunities and hard problems that are critical to get right for society to benefit from AI. Invitation only.
- [SHfHS – Saving Humanity from Homo Sapiens](http://shfhs.org/) - Small organization with a long history of funding x-risk reduction.
- [Superlinear Prizes](https://www.super-linear.org/) - Prize platform for x-risk reduction and other EA contests.
- [Nonlinear Network](https://nonlinearnetwork.org/) - Funder network for x-risk reduction.
- [GAIA – Grantmakers for AI Alignment](https://grantmaking.ai/) - Joinable donor circle for people earning to give or allocating funds towards reducing AI x-risk.
- [AI Alignment Awards](https://www.alignmentawards.com/) - Research paper/essay writing competition platform.
- [Preamble Windfall Foundation](https://www.preambleforgood.org/) - Funding organization aiming to minimize the risk of AI systems, created by PreambleAI.
- [Polaris Ventures](https://polaris-ventures.org/) - Grantmaking nonprofit focused on supporting research into how increasingly autonomous systems can cooperate peacefully with one another.
- [NSF – National Science Foundation – Safe Learning-Enabled Systems](https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems) - Funds research into the design and implementation of safe learning-enabled systems in which safety is ensured with high levels of confidence.
- [CAIF – Cooperative AI Foundation](https://www.cooperativeai.com/foundation) - Charity foundation supporting research into cooperative intelligence of advanced AI.

## **Strategy and Governance Research**

- [FHI – Future of Humanity Institute](https://www.fhi.ox.ac.uk/) - Oxford-based longtermist/x-risk research organization led by Nick Bostrom.
- [FLI – Future of Life Institute](https://futureoflife.org/) - Outreach, policy, grantmaking, and event organization for x-risks, including AI safety.
- [GovAI – Centre for the Governance of AI](https://www.governance.ai/) - AI governance research group at Oxford, advising government, industry, and civil society.
- [CLR – Center for Long-Term Risk](https://longtermrisk.org/) - Research, grants, and community-building around AI safety, focused on conflict scenarios as well as technical and philosophical aspects of cooperation.
- [CSET – Center for Security and Emerging Technology](https://cset.georgetown.edu/) - Think tank at Georgetown University, USA, doing policy analysis at the intersection of national and international security and emerging technologies.
- [AIPI – AI Policy Institute](https://theaipi.org/) - Channeling public concern into effective regulation by engaging with policymakers, media, and the public to shape a future where AI is developed responsibly and transparently.
- [CLTR – Center for Long-Term Resilience](https://www.longtermresilience.org/about) - Independent think tank with a mission to transform global resilience to extreme risks.
- [ICFG – International Center for Future Generations](https://icfg.eu/) - European think-and-do tank for improving societal resilience in relation to exponential technologies and x-risks.
- [Manifold Markets](https://manifold.markets/markets?s=relevance&f=open&q=ai+safety) - Play-money prediction markets on many topics, including AI safety.
- [TFI – Transformative Futures Institute](https://transformative.org/) - Research agenda includes raising awareness of the need for urgent action to counter the risks of advanced AI.
- [Metaculus](https://www.metaculus.com/questions/?order_by=-rank&main-feed&search=ai) - Forecasting platform for many topics, including AI.
- [FRI – Forecasting Research Institute](https://forecastingresearch.org) - Advancing the science of forecasting for the public good.
- [QURI – Quantified Uncertainty Research Institute](https://quantifieduncertainty.org/) - Interviews with AI safety researchers.
- [AI Impacts](https://aiimpacts.org/) - Strategic research, e.g., expert surveys and AI forecasting via analogies to other technological developments.
- [Epoch AI](https://epochai.org/) - Research group studying AI forecasting, produced graph showing compute used by every major ML model.
- [Rethink Priorities](https://rethinkpriorities.org/longtermism) - Research organization with an AI Governance and Strategy (AIGS) team as well as an Existential Security Team (XST).
- [Convergence Analysis](https://www.convergenceanalysis.org/) - X-risk research, advises AI organizations on responsible development.
- [TFS – The Future Society](https://thefuturesociety.org/) - Aligning AI through governance, doing policy research, advisory services, seminars/summits, and educational programs.
- [GCRI – Global Catastrophic Risk Institute](https://gcrinstitute.org/) - Small think tank that tries to bridge scholarship, government, and private industry.
- [CSER – Center for the Study of Existential Risk](https://www.cser.ac.uk/research/risks-from-artificial-intelligence/) - Cambridge group doing miscellaneous existential safety research.
- [CFI – Centre for the Future of Intelligence](http://lcfi.ac.uk/) - Interdisciplinary research centre within the University of Cambridge addressing the challenges and opportunities posed by AI.
- [GPI – Global Priorities Institute](https://globalprioritiesinstitute.org/) - Oxford research group focusing mainly on moral philosophy but also conceptual AI alignment.
- [Median Group](http://mediangroup.org/) - Research group working on models of past and future progress in AI, as well as intelligence enhancement and sociology related to x-risks.
- [LPP – Legal Priorities Project](https://legalpriorities.org/research.html) - Conducting legal research that mitigates x-risk and promotes the flourishing of future generations.
- [CAIP – Center for AI Policy](http://aipolicy.us/) - US policy think tank dedicated to effective regulation to mitigate catastrophic risks posed by advanced AI.
- [Future Matters](https://futuremattersproject.org/) - Provides strategy consulting services to clients trying to advance AI safety through policy, politics, coalitions, and/or social movements.
- [PAI – Partnership on AI](https://partnershiponai.org/) - Convening diverse, international stakeholders in order to pool collective wisdom to advance positive outcomes in AI.
- [IAPS – Institute for AI Policy and Strategy](https://www.iaps.ai/) - Research organization focusing on AI regulations, compute governance, international governance & China, and lab governance.

---

## **Conceptual Research**

- [MIRI – Machine Intelligence Research Institute](https://intelligence.org/) - The original AI safety technical research organization, doing agent foundations/ conceptual work, founded by Eliezer Yudkowsky.
- [ARC – Alignment Research Center](https://alignment.org/) - Research organization led by Paul Christiano, doing model evaluations and theoretical research focusing on the Eliciting Latent Knowledge (ELK) problem.
- [Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism) - Accelerating alignment progress by extending human cognition with AI.
- [Orthogonal](https://orxl.org) - Formal alignment organization led by Tamsin Leake, focused on agent foundations.
- [ALTER – Association for Long-Term Existence and Resilience](https://alter.org.il/) - Research organization doing work on infra-bayesianism (led by Vanessa Kosoy) and on policy for bio and AI risks (led by David Manheim).
- [John Wentworth](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans) - Independent researcher working on selection theorems, abstraction, and agency.
- [CHAI – Center for Human-Compatible AI](https://humancompatible.ai/) - Academic AI safety research organization led by Stuart Russell in Berkeley.
- [Obelisk – Brain-like-AGI Safety](https://astera.org/obelisk/) - Lab building towards aligned AGI that looks like human brains. Featuring Steve Byrnes.
- [Team Shard](https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview) - Independent researchers trying to find reward functions that reliably instill certain values in agents.
- [ACS – Alignment of Complex Systems](https://acsresearch.org/) - Focused on conceptual work on agency and the intersection of complex systems and AI alignment, based at Charles University, Prague.
- [PAISRI – Phenomenological AI Safety Research Institute](https://paisri.org/) - Performs and encourages AI safety research using phenomenological methods.
- [Eli Lifland](https://www.elilifland.com/) - Independent researcher.
- [Dylan Hadfield-Menell](https://people.csail.mit.edu/dhm/) - Assistant professor at MIT working on agent alignment.
- [Jacob Steinhardt](https://jsteinhardt.stat.berkeley.edu/) - Assistant professor in Statistics at UC Berkeley working on alignment.
- [Roman Yampolskiy](http://cecs.louisville.edu/ry/) - Author of two books on AI safety, Professor at the University of Louisville, background in cybersecurity.

---

## **Applied Research**

- [DeepMind](https://www.deepmind.com/) - Leading AI capabilities organization with a strong safety team, based in London.
- [OpenAI](https://openai.com/) - San Francisco-based AI research lab led by Sam Altman, created ChatGPT.
- [Anthropic](https://www.anthropic.com/) - AI research lab focusing on LLM alignment (particularly interpretability), featuring Chris Olah, Jack Clark, and Dario Amodei.
- [CAIS – Center for AI Safety](https://safe.ai/) - Research and field-building nonprofit doing technical research and ML safety advocacy.
- [Conjecture](https://www.conjecture.dev/) - Alignment startup whose work includes interpretability, epistemology, and developing a theory of LLMs.
- [Redwood Research](https://www.redwoodresearch.org/) - Researching interpretability and aligning LLMs.
- [EleutherAI](https://www.eleuther.ai/) - A hacker collective focused on open-source ML and alignment research. Best alignment memes channel on the internet.
- [Ought](https://ought.org/) - Working on factored-cognition research assistants, e.g., Ought.
- [Aligned AI](https://buildaligned.ai/company) - An Oxford-based startup working on safe off-distribution generalization, featuring Stuart Armstrong.
- [FAR AI](https://alignmentfund.org/) - Ensuring AI systems are trustworthy and beneficial to society by incubating and accelerating research agendas too resource-intensive for academia but not yet ready for commercialization.
- [Encultured](https://www.encultured.ai/) - A video game company focused on enabling the safe introduction of AI technologies into gaming.
- [Apollo Research](https://www.apolloresearch.ai/) - Evals for interpretability and behavior that aim to detect deception.
- [Timaeus](https://timaeus.co/) - Research organization focused on singular learning theory and developmental interpretability.
- [Cavendish Labs](https://cavendishlabs.org/) - An AI safety research lab in Vermont, USA.
- [David Krueger](https://www.davidscottkrueger.com/) - Runs an alignment lab at the University of Cambridge.
- [Sam Bowman](https://cims.nyu.edu/~sbowman/) - Associate Professor at New York University working on aligning LLMs.
- [Modeling Cooperation](https://www.modelingcooperation.com/) - Researching AI competition dynamics and building research tools.
- [AOI – AI Objectives Institute](https://ai.objectives.institute/) - Research lab creating tools and projects to keep human values central to AI impact, aiming to avoid catastrophe while improving flourishing.

---

## **Training and Education**

- [AISF – AI Safety Fundamentals](https://aisafetyfundamentals.com/) - The standard introductory courses. 3 months long, 3 tracks: alignment, governance, and alignment 201.
- [MATS – ML Alignment & Theory Scholars Program](https://www.matsprogram.org/) - 2 weeks training, 8 weeks onsite (mentored) research, and, if selected, 4 months extended research.
- [AI Safety Camp](https://aisafety.camp/) - 3-month online research program with mentorship.
- [ERA – Existential Risk Alliance – Fellowship](https://erafellowship.org/) - In-person paid 8-week summer fellowship in Cambridge.
- [CHERI – Swiss Existential Risk Initiative – Summer Fellowship](https://www.xrisk.ch/research-fellowship) - Expanding and coordinating global catastrophic risk mitigation efforts in Switzerland.
- [Alignment Jam](https://alignmentjam.com/jams) - Regular hackathons around the world for people getting into AI safety.
- [PIBBSS – Principles of Intelligent Behavior in Biological and Social Systems – Summer Research Fellowship](https://www.pibbss.ai/fellowship) - Fellowship for young researchers studying complex and intelligent behavior in natural and social systems.
- [SAIL – Safe AI London](https://www.safeailondon.org/) - Events and training programs in London.
- [Intro to MLS – ML Safety](https://course.mlsafety.org/) - Virtual syllabus for ML safety.
- [GCP – Global Challenges Project](https://www.globalchallengesproject.org/) - Intensive 3-day workshops for students to explore x-risk reduction.
- [HAIST – Harvard AI Safety Team](https://haist.ai/) - Harvard student group.
- [MAIA – MIT AI Alignment](https://www.mitalignment.org/) - MIT student group.
- [OxAI – Oxford AI – Society](https://www.oxai.org/ai-safety) - Oxford student group.
- [ASH – AI Safety Hub](https://www.aisafetyhub.org/) - Supports projects in AIS movement building.
- [BudAI – Budapest AI Safety](https://www.budapestaisafety.org/) - Local AI Safety group in Budapest.
- [AISHED – AI Safety Hub Edinburgh](https://aished.org) - Community of people interested in ensuring that AI benefits humanity’s long-term future.
- [CBAI – Cambridge Boston Alignment Initiative](https://www.cbai.ai/) - Boston organization for helping students get into AI safety via workshops and bootcamps. Supports HAIST and MAIA.
- [CLR – Center on Long-Term Risk – Summer Research Fellowship](https://longtermrisk.org/summer-research-fellowship/) - 2–3-month summer research fellowship in London working on reducing long-term future suffering.
- [HA – Human-aligned AI – Summer School](https://humanaligned.ai/) - Program for teaching research methodology.
- [CHAI – Center for Human-Compatible AI – Internship](https://humancompatible.ai/jobs#chai-internships) - Research internship at UC Berkeley.
- [ARENA – Alignment Research Engineer Accelerator](https://www.arena.education/) - Technical upskilling program in London with a focus on LLM alignment.
- [SERI – Stanford Existential Risk Initiative – Fellowship](https://seri.stanford.edu/fellowships) - 10-week funded, mentored summer research fellowship for undergrad and grad students (primarily at Stanford).
- [AISI – AI Safety Initiative at Georgia Tech](https://aisi.dev) - Georgia Tech community doing bootcamps, seminars, and research.
- [SAIA – Stanford AI Alignment](https://stanfordaialignment.org/) - AI safety student group at Stanford. Accelerating students into careers in AI safety and building the alignment community at Stanford.
- [EffiSciences](https://ia.effisciences.org/) - French student collective doing hackathons, conferences, etc.
- [MLAB – Machine Learning for Alignment Bootcamp](https://www.redwoodresearch.org/mlab) - Bootcamp aimed at teaching ML relevant to doing alignment research. Run by Redwood Research.
- [WAISI – Wisconsin AI Safety Initiative](https://win.wisc.edu/organization/waisi) - Wisconsin student group dedicated to reducing AI risk through alignment and governance.
- [Training For Good – EU Tech Policy Fellowship](https://www.trainingforgood.com/europe-tech-policy) - An 8-month program for ambitious graduates intent on careers to improve EU policy on emerging technology.
- [AISG – AI Safety Initiative Groningen](https://www.aisig.org/) - Student group in Groningen, Netherlands.

---

## **Research Support**

- [Lightcone Infrastructure](https://www.lightconeinfrastructure.com/) - Maintains LessWrong and the Alignment Forum and a funding allocation system.
- [Nonlinear Fund](https://www.nonlinear.org/) - "Means-neutral" AI safety organization, doing miscellaneous stuff, including offering bounties on small-to-large AI safety projects and maintaining the Nonlinear Library podcast.
- [CEEALAR – Centre for Enabling EA Learning & Research](https://ceealar.org/) (formerly EA Hotel) - Free or subsidized accommodation and catering in Blackpool, UK, for people working on AI safety and other EA cause areas.
- [AED – Alignment Ecosystem Development](https://alignment.dev/) - Building infrastructure for the alignment ecosystem (volunteers welcome).
- [Apart Research](https://apartresearch.com/) - Runs alignment hackathons and provides AI safety updates and ideas.
- [Arb Research](https://arbresearch.com/) - Consultancy for forecasting, machine learning, and epidemiology, doing original research, evidence reviews, and large-scale data pipelines.
- [BERI – Berkeley Existential Risk Initiative](https://existence.org/) - "Operations as a service" to alignment researchers, especially in academia.
- [SERI – Stanford Existential Risks Initiative](https://seri.stanford.edu/) - Runs research fellowships, an annual conference, speaker events, discussion groups, and a frosh-year COLLEGE class.
- [Impact Ops](https://impact-ops.org/) - Operations support for EA projects.
- [GPAI – Global Partnership on AI](https://gpai.ai/) - International initiative aiming to bridge the gap between theory and practice on AI by supporting research and applied activities on AI-related priorities.
- [ENAIS – European Network for AI Safety](https://enais.co/) - Connecting researchers and policymakers for safe AI in Europe.
- [Constellation](https://www.constellation.org/) - Berkeley research center growing and supporting the ecosystem of people working to ensure the safety of powerful AI systems.
- [LISA – London Initiative for Safe AI](https://www.safeai.org.uk/) - Co-working space hosting independent researchers, organizations (including BlueDot Impact, Apollo, Leap Labs), and upskilling & acceleration programs (including MATS, ARENA).

---

## **Career Support**
- [AIS (AI Safety) Quest](https://aisafety.quest/) - Helps people navigate the AI safety space with a welcoming human touch, offering personalized guidance and fostering collaborative study and project groups.
- [AISS – AI Safety Support](https://www.aisafetysupport.org/) - Free calls to advise on working on AI safety, Slack channel, resources list, and newsletter for AI safety opportunities.
- [80,000 Hours – Career Guide](https://80000hours.org/problem-profiles/artificial-intelligence/) - Article with motivation and advice for pursuing a career in AI safety.
- [Successif](http://successif.org/) - Helps mid-career and senior professionals transition into AI safety, performing market research and providing a range of services, including coaching, mentoring, and training.
- [AI Safety Google Group](https://groups.google.com/g/david-kruegers-80k-people) (formerly 80,000 Hours AI Safety Group) - Updates on academic posts, grad school, and training relating to AI safety.
- [80,000 Hours – Job Board](https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy) - Comprehensive list of job postings related to AI safety and governance.
- [Effective Thesis – Academic Opportunities](https://effectivethesis.org/thesis-topics/human-aligned-ai/#:~:text=Academic%20research%20groups-,Find%20a%20thesis%20topic,-If%20you%E2%80%99re%20interested) - Lists thesis topic ideas in AI safety and coaches people working on them.
- [Effective Thesis – Early Career Research Opportunities](https://airtable.com/appTMKkmier3P8nGv/shrvjOAM4V7AcCCVo/tbl9t9mP1A37OVhzy/viw2uGZu2li1jPc9q) - Lists academic career opportunities for early-stage researchers (jobs, bootcamps, internships).
- [Nonlinear – Coaching for AI Safety Entrepreneurs](https://www.nonlinear.org/coaching.html) - Free coaching for people running an AI safety startup or considering starting one.
- [Nonlinear – Career Advice for AI Safety Entrepreneurs](http://www.nonlinear.org/career-advice) - Free career advice for people considering starting an AI safety org (technical, governance, meta, for-profit, or non-profit).

---

## **Resources**

- [How to pursue a career in technical AI alignment](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment) - A guide for people who are considering direct work on technical AI alignment.
- [Stampy's AI Safety Info](http://aisafety.info/) - Interactive FAQ; Single-Point-Of-Access into AI safety.
- [AI Safety Training](https://aisafety.training/) - List of all available training programs, conferences, hackathons, and events.
- [AI Safety Ideas](https://aisafetyideas.com/) - Repository of possible research projects and testable hypotheses.
- [AI Safety Communities](https://aisafety.community/) - Repository of AI safety communities.
- [AI Safety Groups Directory](https://aisafetyfundamentals.com/local-groups) - Curated directory of AI safety groups (local and online).
- [AI Watch](https://aiwatch.issarice.com/) - Database of AI safety research agendas, people, organizations, and products.
- [AI Plans](https://ai-plans.com/) - Ranked & scored contributable compendium of alignment plans and their problems.
- [AI Risk Discussions](https://ai-risk-discussions.org/) - Interactive walkthrough of core AI x-risk arguments and transcripts of conversations with AI researchers.
- [AI Safety Info Distillation Fellowship](https://docs.google.com/document/d/1AGo3bUAZnCoLj2a_TtgU5a-x7txIrNJt50QuouO8MVc/edit) - 3-month paid fellowship to write content for Stampy's AI Safety Info.
- [OpenBook](https://openbook.fyi/) - Tracker for EA donations.
- [Donations List](https://donations.vipulnaik.com/?cause_area_filter=AI+safety) - Website tracking donations to AI safety.
- [Map of AI Existential Safety](https://aisafety.world) - To understand recursion, one must first understand recursion.
- [AI Safety Map Anki Deck](https://ankiweb.net/shared/info/1103716634) - Helps you learn and memorize the main organizations, projects, and programs currently operating in the AI safety space.

---

## **Media**

- [AI Explained](https://www.youtube.com/@aiexplained-official/videos) - Not explicitly on AI safety, but x-risk aware and introduces stuff well.
- [80,000 Hours Podcast](https://80000hours.org/topic/careers/top-recommended-careers/technical-ai-safety-research/?content-type=podcast) - Interviews on pursuing a career in AI safety.
- [AISS – AI Safety Support – Newsletter](https://www.aisafetysupport.org/newsletter) - Lists opportunities in alignment.
- [ML & AI Safety Updates](https://www.lesswrong.com/posts/uRosq4YtNiZxywcAq/newsletter-for-alignment-research-the-ml-safety-updates) (Apart Research) - Weekly podcast, YouTube, and newsletter with updates on AI safety.
- [EA – Effective Altruism – Talks](https://www.eatalks.org/) - Podcast discussing AI safety and other EA topics.
- [Nonlinear Library](https://forum.effectivealtruism.org/posts/JTZTBienqWEAjGDRv/listen-to-more-ea-content-with-the-nonlinear-library) - Podcast of text-to-speech readings of top EA and LessWrong content.
- [The Inside View](https://www.youtube.com/c/TheInsideView) - Meme-y interviews with AI safety researchers with a focus on short timelines.
- [Rational Animations](https://www.youtube.com/channel/UCgqt1RE0k0MIr0LoyJRy2lg) - Animations about EA, rationality, the future of humanity, and AI safety.
- [AI Safety Videos](http://aisafety.video/) - Comprehensive index of AI safety video content.
- [ML Safety Newsletter](https://newsletter.mlsafety.org/) - Monthly-ish newsletter by Dan Hendrycks focusing on applied AI safety and ML.
- [ImportAI Newsletter](https://jack-clark.net/) - Weekly developments in AI (incl. governance), with bonus short stories.
- [ERO – Existential Risk Observatory](https://www.existentialriskobservatory.org/unaligned-ai/) - Focused on improving coverage of x-risks in mainstream media.
- [AXRP – AI X-risk Research Podcast](https://axrp.net/) - Podcast about technical AI safety.
- [FLI – Future of Life Institute – Podcast](https://futureoflife.org/podcast/) - Interviews with AI safety researchers and other x-risk topics.
- [PauseAI](https://pauseai.info) - Campaign group dedicated to calling for an immediate global moratorium on AGI development.
- [Stop AI](http://stop.ai/) - Website communicating the risks of god-like AI to the public and offering proposals.
- [GAIM – Global AI Moratorium](https://moratorium.ai) - Aiming to establish a global moratorium on AI until alignment is solved.
- [CAS – Campaign for AI Safety](http://campaignforaisafety.org/) - Campaign group dedicated to increasing public understanding of AI safety and calling for strong laws to stop the development of dangerous and powerful AI.
- [SaferAI](http://safer-ai.org/) - Developing the auditing infrastructure for general-purpose AI systems, particularly LLMs.

---

## **Blogs**

- [LessWrong](https://www.lesswrong.com/) - Online forum dedicated to improving human reasoning often has AI safety content.
- [EA – Effective Altruism – Forum](https://forum.effectivealtruism.org/) - Forum on doing good as effectively as possible, including AI safety.
- [AF – Alignment Forum](https://www.alignmentforum.org/) - Central discussion hub for AI safety. Most AI safety research is published here.
- [Cold Takes](https://www.cold-takes.com/) - Blog about transformative AI, futurism, research, ethics, philanthropy, etc., by Holden Karnofsky.
- [ACX – Astral Codex Ten](https://www.astralcodexten.com/) - A blog about many things, including summaries and commentary on AI safety.
- [Arbital](https://arbital.greaterwrong.com/explore/ai_alignment/) - Wiki on AI alignment theory, mostly written by Eliezer Yudkowsky.
- [janus's Blog](https://generative.ink/) - Generative.ink, the blog of janus the GPT cyborg.
- [Victoria Krakovna's Blog](https://vkrakovna.wordpress.com/blog/) - Blog by a research scientist at DeepMind working on AGI safety.
- [Bounded Regret – Jacob Steinhardt's Blog](https://bounded-regret.ghost.io/) - UC Berkeley statistics prof blog on ML safety.
- [Paul Christiano’s Blog](https://ai-alignment.com/) - Blog on aligning prosaic AI by one of the leading AI safety researchers.
- [DeepMind Safety Research](https://deepmindsafetyresearch.medium.com/) - Safety research from DeepMind (hybrid academic/commercial lab).
- [Episomological Vigilance – Adam Shimi's Blog](https://epistemologicalvigilance.substack.com/) - Blog by Conjecture researcher working on improving epistemics for AI safety.
- [Carado.moe](https://carado.moe/) - Blog about AI risk, alignment, etc., by an agent foundations researcher.
- [AIZI – from AI to ZI](https://aizi.substack.com/) - Blog on AI safety work by a math PhD doing freelance research.
- [Daniel Paleka – AI safety takes](https://substack.com/profile/94598084-daniel-paleka) - Monthly+ newsletter on AI safety happenings.
- [Vox – Future Perfect](https://www.vox.com/future-perfect) - Index of Vox articles, podcasts, etc., around finding the best ways to do good.