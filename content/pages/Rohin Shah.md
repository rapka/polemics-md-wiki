---
tags:
  - People
  - AI
  - DeepMind
  - Rationality
  - EffectiveAltruism
aliases:
  - Alignment Newsletter
---
# Rohin Shah

Works on the safety team at [DeepMind](DeepMind.md). Did his PhD at [CHAI](CHAI.md). Used to write the Alignment Newsletter.

https://forum.effectivealtruism.org/users/rohinmshah

https://rohinshah.com/


https://80000hours.org/podcast/episodes/rohin-shah-deepmind-doomers-and-doubters/#is-it-time-to-slow-down-013325

>**Rob Wiblin:** It seems like, given that we’re extremely uncertain about how difficult the problem is, it makes the most sense to act as if the difficulty is somewhere in the middle. Because if the problem is virtually impossible to solve, then probably we’re just screwed no matter what. If it’s really trivial to solve, it also doesn’t really matter that much what we do. Whereas if it’s in the middle, then putting a bit more effort into this, or being a bit smarter about what efforts we engage in, might make the difference between things going better or worse. Does that reasoning make sense?

>**Rohin Shah:** I would say yes, that reasoning does make sense. I would prefer to frame it a bit differently. The way I usually think about it is that every action I take that’s meant to be about solving AI risk, there should be some concrete world, that ideally I could describe, in which this makes the difference between doom and not doom.
>  
Now, for any given action, especially small-scale actions, it’s going to be an extremely, extremely detailed and implausible concrete story — something where you’re like, “What? That’s definitely not going to happen.” One in a billion or whatever. That’s fine. But there should be some world in which you can make this sort of story.

>It doesn’t have to be via technical things; it could be like: “Because I did this 80K podcast, some brilliant ML researcher out there listened to it on a whim and was convinced to work on alignment. And they came up with this brilliant idea that, when combined with all of the other techniques, meant that the first lab to build an x-risky system ended up building one that didn’t cause doom instead of one that did cause doom.” Yeah, that’s an extremely conjunctive, implausibly concrete chain of events. But also, obviously the chance that this particular podcast is going to be the difference between doom and not doom is obviously extremely tiny.