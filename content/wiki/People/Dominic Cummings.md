# Dominic Cummings

https://dominiccummings.substack.com/p/reading-list?utm_source=%2Fsearch%2Frationalist&utm_medium=reader2

>[Yudkowsky’s](https://www.lesswrong.com/tag/sequences) _[Sequences](https://www.lesswrong.com/tag/sequences)_, the core document of the ‘rationalist’ movement. And a very recent post, _[AGI Ruin](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)_, summarising the arguments on why artificial general intelligence is so dangerous and why controlling these dangers is so very hard.

>[Follow Zvi.](https://thezvi.wordpress.com/2017/09/10/best-of-dont-worry-about-the-vase/) He’s a very unusual thinker and much more right much more often than just about anybody, partly because of how he thinks. If you read his blogs and trusted him on covid over the entire CDC/FDA/WHO bureaucracies, you’d have come out far ahead. He replied to Yudkowsky’s AGI ruin [here](https://www.lesswrong.com/posts/LLRtjkvh9AackwuNB/on-a-list-of-lethalities) as did Paul Christiano [here](https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer). (NB. You often read versions of ‘those really at the edge of this research do not predict fast timetables’. This is false. I know some of them. Those who think very fast timetables are plausible _do not talk about it publicly_ because (partly) they worry about the effects of their comments. Predictions on AGI can be Straussian. Many academics predicted OpenAI’s approach would not work but have been [proved repeatedly wrong](https://www.gwern.net/Scaling-hypothesis).)

>Follow Julia Galef. Her recent book, _The Scout Mindset_, was excellent.

https://www.cgdev.org/publication/uk-effective-altruist

## About



### Career, Appointments, and Donations

Bla bla bla did _bla bla_ [^3]


### Donations




