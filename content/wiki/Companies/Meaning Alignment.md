# Meaning Alignment

[Website](https://www.meaningalignment.org/)


From their [about page](https://www.meaningalignment.org/)
> The Meaning Alignment Institute is a research organization with the goal of ensuring human flourishing in the age of AGI, through our unique expertise in _meaning_ and _values_.

Co-founder Joe Edelman was also the co-founder of the [Center for Humane Technology]().


### FAQ

https://www.meaningalignment.org/faq

> **Does this matter after superintelligence?**
> We believe it is possible to build ASW (Artificial Superwisdom) – systems that are wiser than any one person alive, that can find win-win solutions we didn’t know existed. Perhaps it is even necessary to face the challenges of the 21st century.
> Our work on moral graphs could theoretically be scaled to superwisdom – systems that develop new values, like we do, through “moral self-play”.
> How to do this in practice is an open research question.


> **Won't a superintelligence automatically be superwise?**
> We don’t know yet, but it is possible. However, we still need to have a good conception of superwisdom to verify this. It is also likely that, just as current LLMs have a good map of human wisdom from reading massive amounts of text but still need RLHF to act upon it, superintelligence might need direction to act upon its latent wisdom.


> **What are you doing, concretely?**
> **Wise AI**

>Sourcing values, [fine-tuning models](https://meaningalignment.substack.com/p/introducing-democratic-fine-tuning) on our notion of wisdom and values, researching systems that extrapolate and build their own moral graphs.

>**Helping society adapt to AI**

>Researching LLM/market hybrids, building a tool that elicits values from people and coordinates based on what they find meaningful, and running events where this tool is used.