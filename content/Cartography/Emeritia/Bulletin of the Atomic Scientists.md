---
tags:
  - X-Risk
  - WikipediaWarnings
  - AI
  - Map2023
  - Emeritia
---
# Bulletin of the Atomic Scientists

[Website](https://thebulletin.org/) | [Wikipedia](https://en.wikipedia.org/wiki/Bulletin_of_the_Atomic_Scientists) |  [Twitter](https://twitter.com/BulletinAtomic)

The Bulletin of the Atomic Scientists is a long running non-profit and academic journal focused on existential risks. As the name implies,, its original focus was on nuclear weapons, but it has since expanded into raising awareness in other areas.

Bizarrely, the Bulletin published a piece called [Why a nuclear weapons ban would threaten, not save, humanity](https://thebulletin.org/2024/01/why-a-nuclear-weapons-ban-would-threaten-not-save-humanity/) in February 2024.

### AI X-Risk 

The Bulletin has begun covering AI more in recent years, including existential risk (filed under the "disruptive technologies" header). A [December 2023 article](https://thebulletin.org/2023/12/policy-makers-should-plan-for-superintelligent-ai-even-if-it-never-happens/)argued that politicians should prepare for an apocalyptic AGI scenario, no matter how unlikely it is. Author Zachary Kallenborn asserted the following:

>A superintelligence with access to the Internet and all published human material would potentially tap into almost every human thought—including the worst of thought. Exposed to the works of the Unabomber, Ted Kaczynski, it might conclude the industrial system is a form of modern slavery, robbing individuals of important freedoms. It could conceivably be influenced by [Sayyid Qutb](https://www.nytimes.com/2003/03/23/magazine/the-philosopher-of-islamic-terror.html), who provided the philosophical basis for al-Qaeda, or perhaps by Adolf Hitler’s Mein Kampf, now in the [public domain](https://www.theatlantic.com/international/archive/2015/12/mein-kampf-copyright-expiration/422364/).

## Next Generation Initiative

# Doomsday Clock
## About

### Leadership

## Finances


[Policy makers should plan for superintelligent AI, even if it never happens](https://thebulletin.org/2023/12/policy-makers-should-plan-for-superintelligent-ai-even-if-it-never-happens/)